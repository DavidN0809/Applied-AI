{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import to_categorical  # for one-hot encoding (optional)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "\n",
    "# Image size\n",
    "img_width, img_height, img_depth = 224, 224, 3\n",
    "data_dir = r'C:\\Users\\alway\\Downloads\\dogs-vs-cats\\train'\n",
    "test_dir = r'C:\\Users\\alway\\Downloads\\dogs-vs-cats\\test1'\n",
    "\n",
    "\n",
    "# Load the dataset and split it into train and validation sets\n",
    "all_images = os.listdir(data_dir)\n",
    "cat_images = [img for img in all_images if 'cat' in img]\n",
    "dog_images = [img for img in all_images if 'dog' in img]\n",
    "\n",
    "# Create dataframe for ease of use\n",
    "data = pd.DataFrame({\n",
    "    'Image': cat_images + dog_images,\n",
    "    'Class': ['cat'] * len(cat_images) + ['dog'] * len(dog_images)\n",
    "})\n",
    "\n",
    "# Shuffle and split dataset\n",
    "df_train, df_valid = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Determine number of unique classes dynamically\n",
    "class_names = np.unique(data['Class'])\n",
    "num_classes = len(class_names)\n",
    "batch_size = 32\n",
    "train_valid_split = 0.2\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "freq = 20\n",
    "nb_train_samples = int(len(df_train) * (1-train_valid_split))\n",
    "nb_valid_samples = len(df_train) - nb_train_samples\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights for balancing dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(data['Class']),\n",
    "    y=data['Class']\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Class mode for binary classification\n",
    "if num_classes < 2:\n",
    "    act_type = 'sigmoid'\n",
    "    class_mode = 'binary'\n",
    "    loss_fun = 'binary_crossentropy'\n",
    "else:\n",
    "    act_type = 'softmax'\n",
    "    class_mode = 'categorical'\n",
    "    loss_fun = 'categorical_crossentropy'\n",
    "\n",
    "print('Type of classification: ', class_mode)\n",
    "print('Loss function: ', loss_fun)\n",
    "print('Activation function: ', act_type)\n",
    "print('Class weights: ', class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    "    rotation_range = 90,\n",
    "    fill_mode = 'constant')\n",
    "\n",
    "# Train and validation generators\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = df_train,\n",
    "    directory = data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    x_col = 'Image',\n",
    "    y_col = 'Class',\n",
    "    class_mode = class_mode,\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True)\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = df_valid,\n",
    "    directory = data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    x_col = 'Image',\n",
    "    y_col = 'Class',\n",
    "    class_mode = class_mode,\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True)\n",
    "\n",
    "# Preview the augmented data\n",
    "X_preview, y_preview = train_generator.next()\n",
    "\n",
    "for k in range(1, 7):\n",
    "    sample_img = X_preview[k, :, :, :]\n",
    "    plt.subplot(2, 3, k)\n",
    "    plt.imshow(sample_img)\n",
    "plt.suptitle('Sample Data Augmentation', fontsize=16)    \n",
    "plt.show()\n",
    "\n",
    "print('y Labels: ', y_preview[1:5])\n",
    "print('sample img, max value: ', np.max(sample_img))\n",
    "print('sample img, min value: ', np.min(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import applications\n",
    "\n",
    "# Build VGG16\n",
    "image_input = Input(shape=(img_width, img_height, img_depth))\n",
    "base_model = applications.vgg16.VGG16(input_tensor=image_input,\n",
    "                         include_top=False,\n",
    "                         weights='imagenet') \n",
    "\n",
    "base_output = base_model.output\n",
    "\n",
    "# Custom layers\n",
    "# BatchNormalization applied after the base model\n",
    "batch1 = BatchNormalization(axis=1)(base_output)  # Correct axis for BatchNorm\n",
    "flat1 = Flatten()(batch1)\n",
    "fc1 = Dense(4096, activation='relu')(flat1)\n",
    "dropfc1 = Dropout(0.5)(fc1)\n",
    "batch2 = BatchNormalization(axis=-1)(dropfc1)  # Ensure correct axis for your backend (TensorFlow uses -1)\n",
    "fc2 = Dense(4096, activation='relu')(batch2)\n",
    "dropfc2 = Dropout(0.5)(fc2)\n",
    "\n",
    "output = Dense(num_classes, activation=act_type)(dropfc2)\n",
    "\n",
    "for layer in base_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=image_input, outputs=output)\n",
    "\n",
    "# Check the number of parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "# Initialize time and directory path\n",
    "init_time = datetime.now()\n",
    "current_time = init_time.strftime('%Y%m%d_%H%M%S')\n",
    "name_dir = r'.\\models\\trained_models_' + current_time + '_fold_num' + str(fold_num)\n",
    "\n",
    "# Create the directory, including intermediate directories if necessary\n",
    "os.makedirs(name_dir, exist_ok=True)\n",
    "\n",
    "# Callbacks1: ModelCheckpointer - save only if validation loss improves, every 10 epochs\n",
    "model_file_format = os.path.join(name_dir, 'model_' + str(fold_num) + '_best_model.{epoch:04d}.hdf5')\n",
    "check = ModelCheckpoint(model_file_format, monitor='val_acc', save_best_only=True, save_freq='epoch', verbose=1)\n",
    "\n",
    "# Callbacks2: TensorBoard\n",
    "tensor_check = os.path.join(r'./logs/', current_time + '_train_testsplit' + str(fold_num))\n",
    "tensor_board = TensorBoard(log_dir=tensor_check, write_graph=True)\n",
    "\n",
    "# Callbacks3: EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "# Plot model\n",
    "figure_name = os.path.join(name_dir, 'model_output.png')\n",
    "plot_model(model, figure_name, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Compile model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss=loss_fun, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Define learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "# Train model with learning rate scheduler, only saving model every 10 epochs if it improves\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=nb_valid_samples // batch_size,\n",
    "                    callbacks=[check, tensor_board, reduce_lr, early_stop],\n",
    "                    class_weight=class_weights_dict)\n",
    "\n",
    "early_stop_name = name_dir + '/fold_num_' + str(fold_num) + 'early_stop_model.hdf5'\n",
    "model.save_weights(early_stop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Get validation data and corresponding labels from valid_generator\n",
    "valid_steps = nb_valid_samples // batch_size\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through the validation data\n",
    "for i in range(valid_steps):\n",
    "    X_val, y_val = next(valid_generator)\n",
    "    y_true.append(np.argmax(y_val, axis=1))\n",
    "    predictions = model.predict(X_val)\n",
    "    y_pred.append(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=valid_generator.class_indices)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ImageLoader that ensures all images are 224x224x3\n",
    "class ImageLoader(Sequence):\n",
    "    def __init__(self, dataframe, directory, target_size, batch_size, label_mapping, shuffle=True, num_classes=None, augment=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.directory = directory\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.label_mapping = label_mapping  # Dictionary mapping label strings to integers\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = list(range(len(self.dataframe)))\n",
    "        self.num_classes = num_classes  # If using one-hot encoding\n",
    "        self.augment = augment  # Flag to apply augmentations\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in batch_indices:\n",
    "            img_path = os.path.join(self.directory, self.dataframe.iloc[i]['Image'])\n",
    "            label = self.dataframe.iloc[i]['Class']\n",
    "            \n",
    "            # Map the label to its corresponding integer value\n",
    "            label = self.label_mapping[label]\n",
    "\n",
    "            # Load the image and resize to target size (224, 224)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (self.target_size[1], self.target_size[0]))  # Resize to target size (224, 224)\n",
    "\n",
    "            # Normalize the image\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            # Check and ensure that the image has 3 channels (RGB)\n",
    "            if img.shape[-1] != 3:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB if necessary\n",
    "\n",
    "            # Apply augmentations if enabled\n",
    "            if self.augment:\n",
    "                img = self.apply_augmentations(img)\n",
    "\n",
    "            # Ensure the image is resized back to target size after augmentations\n",
    "            img = cv2.resize(img, (self.target_size[1], self.target_size[0]))\n",
    "\n",
    "            # Convert label to a scalar or one-hot encoded format\n",
    "            if self.num_classes:\n",
    "                # One-hot encoding for categorical labels\n",
    "                label = to_categorical(label, num_classes=self.num_classes)\n",
    "            else:\n",
    "                label = np.array([label])  # For binary classification, wrap scalar labels in an array\n",
    "\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "\n",
    "    def apply_augmentations(self, img):\n",
    "        # Random rotation\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.randint(-30, 30)\n",
    "            M = cv2.getRotationMatrix2D((self.target_size[1] // 2, self.target_size[0] // 2), angle, 1)\n",
    "            img = cv2.warpAffine(img, M, (self.target_size[1], self.target_size[0]))\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "\n",
    "        # Random zoom\n",
    "        if random.random() < 0.5:\n",
    "            scale = random.uniform(0.8, 1.2)\n",
    "            h, w = img.shape[:2]\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            img = cv2.resize(img, (new_w, new_h))\n",
    "            img = img[(new_h - h) // 2:(new_h + h) // 2, (new_w - w) // 2:(new_w + w) // 2]\n",
    "\n",
    "        # Random brightness adjustment\n",
    "        if random.random() < 0.5:\n",
    "            value = random.uniform(0.8, 1.2)\n",
    "            img = np.clip(img * value, 0, 1)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'dog': 0, 'cat': 1}  # Mapping labels to integers\n",
    "num_classes = len(label_mapping)  # Number of classes\n",
    "\n",
    "train_loader = ImageLoader(df_train, data_dir, target_size=(img_width, img_height), batch_size=batch_size, label_mapping=label_mapping, num_classes=num_classes, augment=True)\n",
    "valid_loader = ImageLoader(df_valid, data_dir, target_size=(img_width, img_height), batch_size=batch_size, label_mapping=label_mapping, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview augmented data\n",
    "X_preview, y_preview = train_loader.__getitem__(0)\n",
    "\n",
    "# Debugging output\n",
    "for k in range(1, 7):\n",
    "    sample_img = X_preview[k, :, :, :]\n",
    "    plt.subplot(2, 3, k)\n",
    "    plt.imshow(sample_img)\n",
    "plt.suptitle('Sample Data Augmentation with Custom Loader', fontsize=16)    \n",
    "plt.show()\n",
    "\n",
    "print('y Labels (first 5): ', y_preview[1:5])\n",
    "print('Sample img, max value: ', np.max(sample_img))\n",
    "print('Sample img, min value: ', np.min(sample_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with learning rate scheduler, only saving model every 10 epochs if it improves\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=nb_valid_samples // batch_size,\n",
    "                    callbacks=[check, tensor_board, reduce_lr],\n",
    "                    class_weight=class_weights_dict)\n",
    "\n",
    "early_stop_name = name_dir + '/fold_num_' + str(fold_num) + 'early_stop_model.hdf5'\n",
    "model.save_weights(early_stop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Get validation data and corresponding labels from valid_generator\n",
    "valid_steps = nb_valid_samples // batch_size\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through the validation data\n",
    "for i in range(valid_steps):\n",
    "    X_val, y_val = next(valid_generator)\n",
    "    y_true.append(np.argmax(y_val, axis=1))\n",
    "    predictions = model.predict(X_val)\n",
    "    y_pred.append(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=valid_generator.class_indices)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
